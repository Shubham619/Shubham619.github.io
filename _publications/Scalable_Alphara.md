---
title: "Scaling Transformer Inference through CPU and Memory Optimization"
permalink: /publication/scaling
date: 2023-10-28
venue: 'IEEE Wintechcon'
---

_**Abstract**_ -- This paper aims to identify optimal memory resources for scaling transformer inference efficiently, making large-scale models more feasible and effective in resource-constrained environments. •The proposed approach can be applied to other AI/ML models to enhance their performance and scalability. In the proposed approach, the algorithm identifies the optimal combination of batch size (optimBT) and total instances should be executed in parallel (optimInst) based on real-time memory resource utilization.•Information with respect to memory such as current utilization (meminfo), total available memory (totMem) and others provided as a input to the algorithm.

[Download paper here](TBP)
